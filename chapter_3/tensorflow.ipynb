{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Some Variables\n",
    "x = tf.ones(shape=(2,1))\n",
    "print(x)\n",
    "\n",
    "x = tf.zeros(shape=(4,3))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[-1.4014292 ],\n",
      "       [-1.2144301 ],\n",
      "       [ 0.37615263]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[1.],\n",
      "       [1.],\n",
      "       [1.]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[5.],\n",
      "       [1.],\n",
      "       [1.]], dtype=float32)>\n",
      "tf.Tensor(\n",
      "[[4. 4.]\n",
      " [4. 4.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow Variables\n",
    "v = tf.Variable(initial_value=tf.random.normal(shape=(3,1)))\n",
    "print(v)\n",
    "\n",
    "# Assign a value\n",
    "v.assign(tf.ones((3,1)))\n",
    "print(v)\n",
    "\n",
    "# Assign to subset\n",
    "v[0,0].assign(5)\n",
    "print(v)\n",
    "\n",
    "# Operations between tensors\n",
    "a = tf.ones((2,2))\n",
    "b = tf.square(a)\n",
    "c = tf.sqrt(a)\n",
    "d = b + c\n",
    "e = tf.matmul(a,b)\n",
    "e *= d\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "tf.Tensor(9.8, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# GradientTape\n",
    "input_var = tf.Variable(initial_value=3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    result = tf.square(input_var)\n",
    "gradient = tape.gradient(result, input_var)\n",
    "print(gradient)\n",
    "\n",
    "# Double differentiation\n",
    "time = tf.Variable(initial_value=0.)\n",
    "with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as inner_tape:\n",
    "        position = 4.9 * time**2\n",
    "    speed = inner_tape.gradient(position, time)\n",
    "acceleration = outer_tape.gradient(speed, time)\n",
    "print(acceleration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Linear classifier\n",
    "num_samples_per_class = 1000\n",
    "negative_samples = np.random.multivariate_normal(\n",
    "                    mean = [0,3],\n",
    "                    cov = [[1, 0.5], [0.5,1]],\n",
    "                    size = num_samples_per_class)\n",
    "\n",
    "positive_samples = np.random.multivariate_normal(\n",
    "                    mean = [3,0],\n",
    "                    cov = [[1, 0.5], [0.5,1]],\n",
    "                    size = num_samples_per_class)\n",
    "\n",
    "# All inputs in one array\n",
    "inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)\n",
    "\n",
    "# Generate Labels\n",
    "targets = np.vstack((np.zeros((num_samples_per_class, 1), dtype=\"float32\"),\n",
    "                     np.ones((num_samples_per_class, 1), dtype=\"float32\")))\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(inputs[:,0], inputs[:,1], c=targets[:,0])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0 : 3.3501\n",
      "Loss at step 1 : 0.5977\n",
      "Loss at step 2 : 0.2027\n",
      "Loss at step 3 : 0.1344\n",
      "Loss at step 4 : 0.1167\n",
      "Loss at step 5 : 0.1074\n",
      "Loss at step 6 : 0.0999\n",
      "Loss at step 7 : 0.0932\n",
      "Loss at step 8 : 0.0872\n",
      "Loss at step 9 : 0.0817\n",
      "Loss at step 10 : 0.0766\n",
      "Loss at step 11 : 0.0721\n",
      "Loss at step 12 : 0.0679\n",
      "Loss at step 13 : 0.0641\n",
      "Loss at step 14 : 0.0606\n",
      "Loss at step 15 : 0.0574\n",
      "Loss at step 16 : 0.0545\n",
      "Loss at step 17 : 0.0519\n",
      "Loss at step 18 : 0.0495\n",
      "Loss at step 19 : 0.0473\n",
      "Loss at step 20 : 0.0453\n",
      "Loss at step 21 : 0.0435\n",
      "Loss at step 22 : 0.0418\n",
      "Loss at step 23 : 0.0403\n",
      "Loss at step 24 : 0.0389\n",
      "Loss at step 25 : 0.0377\n",
      "Loss at step 26 : 0.0365\n",
      "Loss at step 27 : 0.0355\n",
      "Loss at step 28 : 0.0345\n",
      "Loss at step 29 : 0.0337\n",
      "Loss at step 30 : 0.0329\n",
      "Loss at step 31 : 0.0321\n",
      "Loss at step 32 : 0.0315\n",
      "Loss at step 33 : 0.0309\n",
      "Loss at step 34 : 0.0303\n",
      "Loss at step 35 : 0.0298\n",
      "Loss at step 36 : 0.0294\n",
      "Loss at step 37 : 0.0290\n",
      "Loss at step 38 : 0.0286\n",
      "Loss at step 39 : 0.0282\n",
      "Loss at step 40 : 0.0279\n",
      "Loss at step 41 : 0.0276\n",
      "Loss at step 42 : 0.0274\n",
      "Loss at step 43 : 0.0271\n",
      "Loss at step 44 : 0.0269\n",
      "Loss at step 45 : 0.0267\n",
      "Loss at step 46 : 0.0265\n",
      "Loss at step 47 : 0.0264\n",
      "Loss at step 48 : 0.0262\n",
      "Loss at step 49 : 0.0261\n",
      "Loss at step 50 : 0.0260\n",
      "Loss at step 51 : 0.0258\n",
      "Loss at step 52 : 0.0257\n",
      "Loss at step 53 : 0.0256\n",
      "Loss at step 54 : 0.0256\n",
      "Loss at step 55 : 0.0255\n",
      "Loss at step 56 : 0.0254\n",
      "Loss at step 57 : 0.0253\n",
      "Loss at step 58 : 0.0253\n",
      "Loss at step 59 : 0.0252\n",
      "Loss at step 60 : 0.0252\n",
      "Loss at step 61 : 0.0251\n",
      "Loss at step 62 : 0.0251\n",
      "Loss at step 63 : 0.0250\n",
      "Loss at step 64 : 0.0250\n",
      "Loss at step 65 : 0.0250\n",
      "Loss at step 66 : 0.0250\n",
      "Loss at step 67 : 0.0249\n",
      "Loss at step 68 : 0.0249\n",
      "Loss at step 69 : 0.0249\n",
      "Loss at step 70 : 0.0249\n",
      "Loss at step 71 : 0.0248\n",
      "Loss at step 72 : 0.0248\n",
      "Loss at step 73 : 0.0248\n",
      "Loss at step 74 : 0.0248\n",
      "Loss at step 75 : 0.0248\n",
      "Loss at step 76 : 0.0248\n",
      "Loss at step 77 : 0.0248\n",
      "Loss at step 78 : 0.0248\n",
      "Loss at step 79 : 0.0247\n",
      "Loss at step 80 : 0.0247\n",
      "Loss at step 81 : 0.0247\n",
      "Loss at step 82 : 0.0247\n",
      "Loss at step 83 : 0.0247\n",
      "Loss at step 84 : 0.0247\n",
      "Loss at step 85 : 0.0247\n",
      "Loss at step 86 : 0.0247\n",
      "Loss at step 87 : 0.0247\n",
      "Loss at step 88 : 0.0247\n",
      "Loss at step 89 : 0.0247\n",
      "Loss at step 90 : 0.0247\n",
      "Loss at step 91 : 0.0247\n",
      "Loss at step 92 : 0.0247\n",
      "Loss at step 93 : 0.0247\n",
      "Loss at step 94 : 0.0247\n",
      "Loss at step 95 : 0.0247\n",
      "Loss at step 96 : 0.0247\n",
      "Loss at step 97 : 0.0247\n",
      "Loss at step 98 : 0.0247\n",
      "Loss at step 99 : 0.0247\n",
      "Loss at step 100 : 0.0247\n",
      "Loss at step 101 : 0.0247\n",
      "Loss at step 102 : 0.0247\n",
      "Loss at step 103 : 0.0247\n",
      "Loss at step 104 : 0.0247\n",
      "Loss at step 105 : 0.0247\n",
      "Loss at step 106 : 0.0247\n",
      "Loss at step 107 : 0.0247\n",
      "Loss at step 108 : 0.0247\n",
      "Loss at step 109 : 0.0247\n",
      "Loss at step 110 : 0.0247\n",
      "Loss at step 111 : 0.0247\n",
      "Loss at step 112 : 0.0247\n",
      "Loss at step 113 : 0.0247\n",
      "Loss at step 114 : 0.0247\n",
      "Loss at step 115 : 0.0247\n",
      "Loss at step 116 : 0.0247\n",
      "Loss at step 117 : 0.0247\n",
      "Loss at step 118 : 0.0247\n",
      "Loss at step 119 : 0.0247\n",
      "Loss at step 120 : 0.0247\n",
      "Loss at step 121 : 0.0247\n",
      "Loss at step 122 : 0.0247\n",
      "Loss at step 123 : 0.0247\n",
      "Loss at step 124 : 0.0247\n",
      "Loss at step 125 : 0.0247\n",
      "Loss at step 126 : 0.0247\n",
      "Loss at step 127 : 0.0247\n",
      "Loss at step 128 : 0.0247\n",
      "Loss at step 129 : 0.0247\n",
      "Loss at step 130 : 0.0247\n",
      "Loss at step 131 : 0.0247\n",
      "Loss at step 132 : 0.0247\n",
      "Loss at step 133 : 0.0247\n",
      "Loss at step 134 : 0.0247\n",
      "Loss at step 135 : 0.0247\n",
      "Loss at step 136 : 0.0247\n",
      "Loss at step 137 : 0.0247\n",
      "Loss at step 138 : 0.0247\n",
      "Loss at step 139 : 0.0247\n",
      "Loss at step 140 : 0.0247\n",
      "Loss at step 141 : 0.0247\n",
      "Loss at step 142 : 0.0247\n",
      "Loss at step 143 : 0.0247\n",
      "Loss at step 144 : 0.0247\n",
      "Loss at step 145 : 0.0247\n",
      "Loss at step 146 : 0.0247\n",
      "Loss at step 147 : 0.0247\n",
      "Loss at step 148 : 0.0247\n",
      "Loss at step 149 : 0.0247\n",
      "Loss at step 150 : 0.0247\n",
      "Loss at step 151 : 0.0247\n",
      "Loss at step 152 : 0.0247\n",
      "Loss at step 153 : 0.0247\n",
      "Loss at step 154 : 0.0247\n",
      "Loss at step 155 : 0.0247\n",
      "Loss at step 156 : 0.0247\n",
      "Loss at step 157 : 0.0247\n",
      "Loss at step 158 : 0.0247\n",
      "Loss at step 159 : 0.0247\n",
      "Loss at step 160 : 0.0247\n",
      "Loss at step 161 : 0.0247\n",
      "Loss at step 162 : 0.0247\n",
      "Loss at step 163 : 0.0247\n",
      "Loss at step 164 : 0.0247\n",
      "Loss at step 165 : 0.0247\n",
      "Loss at step 166 : 0.0247\n",
      "Loss at step 167 : 0.0247\n",
      "Loss at step 168 : 0.0247\n",
      "Loss at step 169 : 0.0247\n",
      "Loss at step 170 : 0.0247\n",
      "Loss at step 171 : 0.0247\n",
      "Loss at step 172 : 0.0247\n",
      "Loss at step 173 : 0.0247\n",
      "Loss at step 174 : 0.0247\n",
      "Loss at step 175 : 0.0247\n",
      "Loss at step 176 : 0.0247\n",
      "Loss at step 177 : 0.0247\n",
      "Loss at step 178 : 0.0247\n",
      "Loss at step 179 : 0.0247\n",
      "Loss at step 180 : 0.0247\n",
      "Loss at step 181 : 0.0247\n",
      "Loss at step 182 : 0.0247\n",
      "Loss at step 183 : 0.0247\n",
      "Loss at step 184 : 0.0247\n",
      "Loss at step 185 : 0.0247\n",
      "Loss at step 186 : 0.0247\n",
      "Loss at step 187 : 0.0247\n",
      "Loss at step 188 : 0.0247\n",
      "Loss at step 189 : 0.0247\n",
      "Loss at step 190 : 0.0247\n",
      "Loss at step 191 : 0.0247\n",
      "Loss at step 192 : 0.0247\n",
      "Loss at step 193 : 0.0247\n",
      "Loss at step 194 : 0.0247\n",
      "Loss at step 195 : 0.0247\n",
      "Loss at step 196 : 0.0247\n",
      "Loss at step 197 : 0.0247\n",
      "Loss at step 198 : 0.0247\n",
      "Loss at step 199 : 0.0247\n",
      "Loss at step 200 : 0.0247\n",
      "Loss at step 201 : 0.0247\n",
      "Loss at step 202 : 0.0247\n",
      "Loss at step 203 : 0.0247\n",
      "Loss at step 204 : 0.0247\n",
      "Loss at step 205 : 0.0247\n",
      "Loss at step 206 : 0.0247\n",
      "Loss at step 207 : 0.0247\n",
      "Loss at step 208 : 0.0247\n",
      "Loss at step 209 : 0.0247\n",
      "Loss at step 210 : 0.0247\n",
      "Loss at step 211 : 0.0247\n",
      "Loss at step 212 : 0.0247\n",
      "Loss at step 213 : 0.0247\n",
      "Loss at step 214 : 0.0247\n",
      "Loss at step 215 : 0.0247\n",
      "Loss at step 216 : 0.0247\n",
      "Loss at step 217 : 0.0247\n",
      "Loss at step 218 : 0.0247\n",
      "Loss at step 219 : 0.0247\n",
      "Loss at step 220 : 0.0247\n",
      "Loss at step 221 : 0.0247\n",
      "Loss at step 222 : 0.0247\n",
      "Loss at step 223 : 0.0247\n",
      "Loss at step 224 : 0.0247\n",
      "Loss at step 225 : 0.0247\n",
      "Loss at step 226 : 0.0247\n",
      "Loss at step 227 : 0.0247\n",
      "Loss at step 228 : 0.0247\n",
      "Loss at step 229 : 0.0247\n",
      "Loss at step 230 : 0.0247\n",
      "Loss at step 231 : 0.0247\n",
      "Loss at step 232 : 0.0247\n",
      "Loss at step 233 : 0.0247\n",
      "Loss at step 234 : 0.0247\n",
      "Loss at step 235 : 0.0247\n",
      "Loss at step 236 : 0.0247\n",
      "Loss at step 237 : 0.0247\n",
      "Loss at step 238 : 0.0247\n",
      "Loss at step 239 : 0.0247\n",
      "Loss at step 240 : 0.0247\n",
      "Loss at step 241 : 0.0247\n",
      "Loss at step 242 : 0.0247\n",
      "Loss at step 243 : 0.0247\n",
      "Loss at step 244 : 0.0247\n",
      "Loss at step 245 : 0.0247\n",
      "Loss at step 246 : 0.0247\n",
      "Loss at step 247 : 0.0247\n",
      "Loss at step 248 : 0.0247\n",
      "Loss at step 249 : 0.0247\n",
      "Loss at step 250 : 0.0247\n",
      "Loss at step 251 : 0.0247\n",
      "Loss at step 252 : 0.0247\n",
      "Loss at step 253 : 0.0247\n",
      "Loss at step 254 : 0.0247\n",
      "Loss at step 255 : 0.0247\n",
      "Loss at step 256 : 0.0247\n",
      "Loss at step 257 : 0.0247\n",
      "Loss at step 258 : 0.0247\n",
      "Loss at step 259 : 0.0247\n",
      "Loss at step 260 : 0.0247\n",
      "Loss at step 261 : 0.0247\n",
      "Loss at step 262 : 0.0247\n",
      "Loss at step 263 : 0.0247\n",
      "Loss at step 264 : 0.0247\n",
      "Loss at step 265 : 0.0247\n",
      "Loss at step 266 : 0.0247\n",
      "Loss at step 267 : 0.0247\n",
      "Loss at step 268 : 0.0247\n",
      "Loss at step 269 : 0.0247\n",
      "Loss at step 270 : 0.0247\n",
      "Loss at step 271 : 0.0247\n",
      "Loss at step 272 : 0.0247\n",
      "Loss at step 273 : 0.0247\n",
      "Loss at step 274 : 0.0247\n",
      "Loss at step 275 : 0.0247\n",
      "Loss at step 276 : 0.0247\n",
      "Loss at step 277 : 0.0247\n",
      "Loss at step 278 : 0.0247\n",
      "Loss at step 279 : 0.0247\n",
      "Loss at step 280 : 0.0247\n",
      "Loss at step 281 : 0.0247\n",
      "Loss at step 282 : 0.0247\n",
      "Loss at step 283 : 0.0247\n",
      "Loss at step 284 : 0.0247\n",
      "Loss at step 285 : 0.0247\n",
      "Loss at step 286 : 0.0247\n",
      "Loss at step 287 : 0.0247\n",
      "Loss at step 288 : 0.0247\n",
      "Loss at step 289 : 0.0247\n",
      "Loss at step 290 : 0.0247\n",
      "Loss at step 291 : 0.0247\n",
      "Loss at step 292 : 0.0247\n",
      "Loss at step 293 : 0.0247\n",
      "Loss at step 294 : 0.0247\n",
      "Loss at step 295 : 0.0247\n",
      "Loss at step 296 : 0.0247\n",
      "Loss at step 297 : 0.0247\n",
      "Loss at step 298 : 0.0247\n",
      "Loss at step 299 : 0.0247\n",
      "Loss at step 300 : 0.0247\n",
      "Loss at step 301 : 0.0247\n",
      "Loss at step 302 : 0.0247\n",
      "Loss at step 303 : 0.0247\n",
      "Loss at step 304 : 0.0247\n",
      "Loss at step 305 : 0.0247\n",
      "Loss at step 306 : 0.0247\n",
      "Loss at step 307 : 0.0247\n",
      "Loss at step 308 : 0.0247\n",
      "Loss at step 309 : 0.0247\n",
      "Loss at step 310 : 0.0247\n",
      "Loss at step 311 : 0.0247\n",
      "Loss at step 312 : 0.0247\n",
      "Loss at step 313 : 0.0247\n",
      "Loss at step 314 : 0.0247\n",
      "Loss at step 315 : 0.0247\n",
      "Loss at step 316 : 0.0247\n",
      "Loss at step 317 : 0.0247\n",
      "Loss at step 318 : 0.0247\n",
      "Loss at step 319 : 0.0247\n",
      "Loss at step 320 : 0.0247\n",
      "Loss at step 321 : 0.0247\n",
      "Loss at step 322 : 0.0247\n",
      "Loss at step 323 : 0.0247\n",
      "Loss at step 324 : 0.0247\n",
      "Loss at step 325 : 0.0247\n",
      "Loss at step 326 : 0.0247\n",
      "Loss at step 327 : 0.0247\n",
      "Loss at step 328 : 0.0247\n",
      "Loss at step 329 : 0.0247\n",
      "Loss at step 330 : 0.0247\n",
      "Loss at step 331 : 0.0247\n",
      "Loss at step 332 : 0.0247\n",
      "Loss at step 333 : 0.0247\n",
      "Loss at step 334 : 0.0247\n",
      "Loss at step 335 : 0.0247\n",
      "Loss at step 336 : 0.0247\n",
      "Loss at step 337 : 0.0247\n",
      "Loss at step 338 : 0.0247\n",
      "Loss at step 339 : 0.0247\n",
      "Loss at step 340 : 0.0247\n",
      "Loss at step 341 : 0.0247\n",
      "Loss at step 342 : 0.0247\n",
      "Loss at step 343 : 0.0247\n",
      "Loss at step 344 : 0.0247\n",
      "Loss at step 345 : 0.0247\n",
      "Loss at step 346 : 0.0247\n",
      "Loss at step 347 : 0.0247\n",
      "Loss at step 348 : 0.0247\n",
      "Loss at step 349 : 0.0247\n",
      "Loss at step 350 : 0.0247\n",
      "Loss at step 351 : 0.0247\n",
      "Loss at step 352 : 0.0247\n",
      "Loss at step 353 : 0.0247\n",
      "Loss at step 354 : 0.0247\n",
      "Loss at step 355 : 0.0247\n",
      "Loss at step 356 : 0.0247\n",
      "Loss at step 357 : 0.0247\n",
      "Loss at step 358 : 0.0247\n",
      "Loss at step 359 : 0.0247\n",
      "Loss at step 360 : 0.0247\n",
      "Loss at step 361 : 0.0247\n",
      "Loss at step 362 : 0.0247\n",
      "Loss at step 363 : 0.0247\n",
      "Loss at step 364 : 0.0247\n",
      "Loss at step 365 : 0.0247\n",
      "Loss at step 366 : 0.0247\n",
      "Loss at step 367 : 0.0247\n",
      "Loss at step 368 : 0.0247\n",
      "Loss at step 369 : 0.0247\n",
      "Loss at step 370 : 0.0247\n",
      "Loss at step 371 : 0.0247\n",
      "Loss at step 372 : 0.0247\n",
      "Loss at step 373 : 0.0247\n",
      "Loss at step 374 : 0.0247\n",
      "Loss at step 375 : 0.0247\n",
      "Loss at step 376 : 0.0247\n",
      "Loss at step 377 : 0.0247\n",
      "Loss at step 378 : 0.0247\n",
      "Loss at step 379 : 0.0247\n",
      "Loss at step 380 : 0.0247\n",
      "Loss at step 381 : 0.0247\n",
      "Loss at step 382 : 0.0247\n",
      "Loss at step 383 : 0.0247\n",
      "Loss at step 384 : 0.0247\n",
      "Loss at step 385 : 0.0247\n",
      "Loss at step 386 : 0.0247\n",
      "Loss at step 387 : 0.0247\n",
      "Loss at step 388 : 0.0247\n",
      "Loss at step 389 : 0.0247\n",
      "Loss at step 390 : 0.0247\n",
      "Loss at step 391 : 0.0247\n",
      "Loss at step 392 : 0.0247\n",
      "Loss at step 393 : 0.0247\n",
      "Loss at step 394 : 0.0247\n",
      "Loss at step 395 : 0.0247\n",
      "Loss at step 396 : 0.0247\n",
      "Loss at step 397 : 0.0247\n",
      "Loss at step 398 : 0.0247\n",
      "Loss at step 399 : 0.0247\n",
      "Loss at step 400 : 0.0247\n",
      "Loss at step 401 : 0.0247\n",
      "Loss at step 402 : 0.0247\n",
      "Loss at step 403 : 0.0247\n",
      "Loss at step 404 : 0.0247\n",
      "Loss at step 405 : 0.0247\n",
      "Loss at step 406 : 0.0247\n",
      "Loss at step 407 : 0.0247\n",
      "Loss at step 408 : 0.0247\n",
      "Loss at step 409 : 0.0247\n",
      "Loss at step 410 : 0.0247\n",
      "Loss at step 411 : 0.0247\n",
      "Loss at step 412 : 0.0247\n",
      "Loss at step 413 : 0.0247\n",
      "Loss at step 414 : 0.0247\n",
      "Loss at step 415 : 0.0247\n",
      "Loss at step 416 : 0.0247\n",
      "Loss at step 417 : 0.0247\n",
      "Loss at step 418 : 0.0247\n",
      "Loss at step 419 : 0.0247\n",
      "Loss at step 420 : 0.0247\n",
      "Loss at step 421 : 0.0247\n",
      "Loss at step 422 : 0.0247\n",
      "Loss at step 423 : 0.0247\n",
      "Loss at step 424 : 0.0247\n",
      "Loss at step 425 : 0.0247\n",
      "Loss at step 426 : 0.0247\n",
      "Loss at step 427 : 0.0247\n",
      "Loss at step 428 : 0.0247\n",
      "Loss at step 429 : 0.0247\n",
      "Loss at step 430 : 0.0247\n",
      "Loss at step 431 : 0.0247\n",
      "Loss at step 432 : 0.0247\n",
      "Loss at step 433 : 0.0247\n",
      "Loss at step 434 : 0.0247\n",
      "Loss at step 435 : 0.0247\n",
      "Loss at step 436 : 0.0247\n",
      "Loss at step 437 : 0.0247\n",
      "Loss at step 438 : 0.0247\n",
      "Loss at step 439 : 0.0247\n",
      "Loss at step 440 : 0.0247\n",
      "Loss at step 441 : 0.0247\n",
      "Loss at step 442 : 0.0247\n",
      "Loss at step 443 : 0.0247\n",
      "Loss at step 444 : 0.0247\n",
      "Loss at step 445 : 0.0247\n",
      "Loss at step 446 : 0.0247\n",
      "Loss at step 447 : 0.0247\n",
      "Loss at step 448 : 0.0247\n",
      "Loss at step 449 : 0.0247\n",
      "Loss at step 450 : 0.0247\n",
      "Loss at step 451 : 0.0247\n",
      "Loss at step 452 : 0.0247\n",
      "Loss at step 453 : 0.0247\n",
      "Loss at step 454 : 0.0247\n",
      "Loss at step 455 : 0.0247\n",
      "Loss at step 456 : 0.0247\n",
      "Loss at step 457 : 0.0247\n",
      "Loss at step 458 : 0.0247\n",
      "Loss at step 459 : 0.0247\n",
      "Loss at step 460 : 0.0247\n",
      "Loss at step 461 : 0.0247\n",
      "Loss at step 462 : 0.0247\n",
      "Loss at step 463 : 0.0247\n",
      "Loss at step 464 : 0.0247\n",
      "Loss at step 465 : 0.0247\n",
      "Loss at step 466 : 0.0247\n",
      "Loss at step 467 : 0.0247\n",
      "Loss at step 468 : 0.0247\n",
      "Loss at step 469 : 0.0247\n",
      "Loss at step 470 : 0.0247\n",
      "Loss at step 471 : 0.0247\n",
      "Loss at step 472 : 0.0247\n",
      "Loss at step 473 : 0.0247\n",
      "Loss at step 474 : 0.0247\n",
      "Loss at step 475 : 0.0247\n",
      "Loss at step 476 : 0.0247\n",
      "Loss at step 477 : 0.0247\n",
      "Loss at step 478 : 0.0247\n",
      "Loss at step 479 : 0.0247\n",
      "Loss at step 480 : 0.0247\n",
      "Loss at step 481 : 0.0247\n",
      "Loss at step 482 : 0.0247\n",
      "Loss at step 483 : 0.0247\n",
      "Loss at step 484 : 0.0247\n",
      "Loss at step 485 : 0.0247\n",
      "Loss at step 486 : 0.0247\n",
      "Loss at step 487 : 0.0247\n",
      "Loss at step 488 : 0.0247\n",
      "Loss at step 489 : 0.0247\n",
      "Loss at step 490 : 0.0247\n",
      "Loss at step 491 : 0.0247\n",
      "Loss at step 492 : 0.0247\n",
      "Loss at step 493 : 0.0247\n",
      "Loss at step 494 : 0.0247\n",
      "Loss at step 495 : 0.0247\n",
      "Loss at step 496 : 0.0247\n",
      "Loss at step 497 : 0.0247\n",
      "Loss at step 498 : 0.0247\n",
      "Loss at step 499 : 0.0247\n",
      "Loss at step 500 : 0.0247\n",
      "Loss at step 501 : 0.0247\n",
      "Loss at step 502 : 0.0247\n",
      "Loss at step 503 : 0.0247\n",
      "Loss at step 504 : 0.0247\n",
      "Loss at step 505 : 0.0247\n",
      "Loss at step 506 : 0.0247\n",
      "Loss at step 507 : 0.0247\n",
      "Loss at step 508 : 0.0247\n",
      "Loss at step 509 : 0.0247\n",
      "Loss at step 510 : 0.0247\n",
      "Loss at step 511 : 0.0247\n",
      "Loss at step 512 : 0.0247\n",
      "Loss at step 513 : 0.0247\n",
      "Loss at step 514 : 0.0247\n",
      "Loss at step 515 : 0.0247\n",
      "Loss at step 516 : 0.0247\n",
      "Loss at step 517 : 0.0247\n",
      "Loss at step 518 : 0.0247\n",
      "Loss at step 519 : 0.0247\n",
      "Loss at step 520 : 0.0247\n",
      "Loss at step 521 : 0.0247\n",
      "Loss at step 522 : 0.0247\n",
      "Loss at step 523 : 0.0247\n",
      "Loss at step 524 : 0.0247\n",
      "Loss at step 525 : 0.0247\n",
      "Loss at step 526 : 0.0247\n",
      "Loss at step 527 : 0.0247\n",
      "Loss at step 528 : 0.0247\n",
      "Loss at step 529 : 0.0247\n",
      "Loss at step 530 : 0.0247\n",
      "Loss at step 531 : 0.0247\n",
      "Loss at step 532 : 0.0247\n",
      "Loss at step 533 : 0.0247\n",
      "Loss at step 534 : 0.0247\n",
      "Loss at step 535 : 0.0247\n",
      "Loss at step 536 : 0.0247\n",
      "Loss at step 537 : 0.0247\n",
      "Loss at step 538 : 0.0247\n",
      "Loss at step 539 : 0.0247\n",
      "Loss at step 540 : 0.0247\n",
      "Loss at step 541 : 0.0247\n",
      "Loss at step 542 : 0.0247\n",
      "Loss at step 543 : 0.0247\n",
      "Loss at step 544 : 0.0247\n",
      "Loss at step 545 : 0.0247\n",
      "Loss at step 546 : 0.0247\n",
      "Loss at step 547 : 0.0247\n",
      "Loss at step 548 : 0.0247\n",
      "Loss at step 549 : 0.0247\n",
      "Loss at step 550 : 0.0247\n",
      "Loss at step 551 : 0.0247\n",
      "Loss at step 552 : 0.0247\n",
      "Loss at step 553 : 0.0247\n",
      "Loss at step 554 : 0.0247\n",
      "Loss at step 555 : 0.0247\n",
      "Loss at step 556 : 0.0247\n",
      "Loss at step 557 : 0.0247\n",
      "Loss at step 558 : 0.0247\n",
      "Loss at step 559 : 0.0247\n",
      "Loss at step 560 : 0.0247\n",
      "Loss at step 561 : 0.0247\n",
      "Loss at step 562 : 0.0247\n",
      "Loss at step 563 : 0.0247\n",
      "Loss at step 564 : 0.0247\n",
      "Loss at step 565 : 0.0247\n",
      "Loss at step 566 : 0.0247\n",
      "Loss at step 567 : 0.0247\n",
      "Loss at step 568 : 0.0247\n",
      "Loss at step 569 : 0.0247\n",
      "Loss at step 570 : 0.0247\n",
      "Loss at step 571 : 0.0247\n",
      "Loss at step 572 : 0.0247\n",
      "Loss at step 573 : 0.0247\n",
      "Loss at step 574 : 0.0247\n",
      "Loss at step 575 : 0.0247\n",
      "Loss at step 576 : 0.0247\n",
      "Loss at step 577 : 0.0247\n",
      "Loss at step 578 : 0.0247\n",
      "Loss at step 579 : 0.0247\n",
      "Loss at step 580 : 0.0247\n",
      "Loss at step 581 : 0.0247\n",
      "Loss at step 582 : 0.0247\n",
      "Loss at step 583 : 0.0247\n",
      "Loss at step 584 : 0.0247\n",
      "Loss at step 585 : 0.0247\n",
      "Loss at step 586 : 0.0247\n",
      "Loss at step 587 : 0.0247\n",
      "Loss at step 588 : 0.0247\n",
      "Loss at step 589 : 0.0247\n",
      "Loss at step 590 : 0.0247\n",
      "Loss at step 591 : 0.0247\n",
      "Loss at step 592 : 0.0247\n",
      "Loss at step 593 : 0.0247\n",
      "Loss at step 594 : 0.0247\n",
      "Loss at step 595 : 0.0247\n",
      "Loss at step 596 : 0.0247\n",
      "Loss at step 597 : 0.0247\n",
      "Loss at step 598 : 0.0247\n",
      "Loss at step 599 : 0.0247\n",
      "Loss at step 600 : 0.0247\n",
      "Loss at step 601 : 0.0247\n",
      "Loss at step 602 : 0.0247\n",
      "Loss at step 603 : 0.0247\n",
      "Loss at step 604 : 0.0247\n",
      "Loss at step 605 : 0.0247\n",
      "Loss at step 606 : 0.0247\n",
      "Loss at step 607 : 0.0247\n",
      "Loss at step 608 : 0.0247\n",
      "Loss at step 609 : 0.0247\n",
      "Loss at step 610 : 0.0247\n",
      "Loss at step 611 : 0.0247\n",
      "Loss at step 612 : 0.0247\n",
      "Loss at step 613 : 0.0247\n",
      "Loss at step 614 : 0.0247\n",
      "Loss at step 615 : 0.0247\n",
      "Loss at step 616 : 0.0247\n",
      "Loss at step 617 : 0.0247\n",
      "Loss at step 618 : 0.0247\n",
      "Loss at step 619 : 0.0247\n",
      "Loss at step 620 : 0.0247\n",
      "Loss at step 621 : 0.0247\n",
      "Loss at step 622 : 0.0247\n",
      "Loss at step 623 : 0.0247\n",
      "Loss at step 624 : 0.0247\n",
      "Loss at step 625 : 0.0247\n",
      "Loss at step 626 : 0.0247\n",
      "Loss at step 627 : 0.0247\n",
      "Loss at step 628 : 0.0247\n",
      "Loss at step 629 : 0.0247\n",
      "Loss at step 630 : 0.0247\n",
      "Loss at step 631 : 0.0247\n",
      "Loss at step 632 : 0.0247\n",
      "Loss at step 633 : 0.0247\n",
      "Loss at step 634 : 0.0247\n",
      "Loss at step 635 : 0.0247\n",
      "Loss at step 636 : 0.0247\n",
      "Loss at step 637 : 0.0247\n",
      "Loss at step 638 : 0.0247\n",
      "Loss at step 639 : 0.0247\n",
      "Loss at step 640 : 0.0247\n",
      "Loss at step 641 : 0.0247\n",
      "Loss at step 642 : 0.0247\n",
      "Loss at step 643 : 0.0247\n",
      "Loss at step 644 : 0.0247\n",
      "Loss at step 645 : 0.0247\n",
      "Loss at step 646 : 0.0247\n",
      "Loss at step 647 : 0.0247\n",
      "Loss at step 648 : 0.0247\n",
      "Loss at step 649 : 0.0247\n",
      "Loss at step 650 : 0.0247\n",
      "Loss at step 651 : 0.0247\n",
      "Loss at step 652 : 0.0247\n",
      "Loss at step 653 : 0.0247\n",
      "Loss at step 654 : 0.0247\n",
      "Loss at step 655 : 0.0247\n",
      "Loss at step 656 : 0.0247\n",
      "Loss at step 657 : 0.0247\n",
      "Loss at step 658 : 0.0247\n",
      "Loss at step 659 : 0.0247\n",
      "Loss at step 660 : 0.0247\n",
      "Loss at step 661 : 0.0247\n",
      "Loss at step 662 : 0.0247\n",
      "Loss at step 663 : 0.0247\n",
      "Loss at step 664 : 0.0247\n",
      "Loss at step 665 : 0.0247\n",
      "Loss at step 666 : 0.0247\n",
      "Loss at step 667 : 0.0247\n",
      "Loss at step 668 : 0.0247\n",
      "Loss at step 669 : 0.0247\n",
      "Loss at step 670 : 0.0247\n",
      "Loss at step 671 : 0.0247\n",
      "Loss at step 672 : 0.0247\n",
      "Loss at step 673 : 0.0247\n",
      "Loss at step 674 : 0.0247\n",
      "Loss at step 675 : 0.0247\n",
      "Loss at step 676 : 0.0247\n",
      "Loss at step 677 : 0.0247\n",
      "Loss at step 678 : 0.0247\n",
      "Loss at step 679 : 0.0247\n",
      "Loss at step 680 : 0.0247\n",
      "Loss at step 681 : 0.0247\n",
      "Loss at step 682 : 0.0247\n",
      "Loss at step 683 : 0.0247\n",
      "Loss at step 684 : 0.0247\n",
      "Loss at step 685 : 0.0247\n",
      "Loss at step 686 : 0.0247\n",
      "Loss at step 687 : 0.0247\n",
      "Loss at step 688 : 0.0247\n",
      "Loss at step 689 : 0.0247\n",
      "Loss at step 690 : 0.0247\n",
      "Loss at step 691 : 0.0247\n",
      "Loss at step 692 : 0.0247\n",
      "Loss at step 693 : 0.0247\n",
      "Loss at step 694 : 0.0247\n",
      "Loss at step 695 : 0.0247\n",
      "Loss at step 696 : 0.0247\n",
      "Loss at step 697 : 0.0247\n",
      "Loss at step 698 : 0.0247\n",
      "Loss at step 699 : 0.0247\n",
      "Loss at step 700 : 0.0247\n",
      "Loss at step 701 : 0.0247\n",
      "Loss at step 702 : 0.0247\n",
      "Loss at step 703 : 0.0247\n",
      "Loss at step 704 : 0.0247\n",
      "Loss at step 705 : 0.0247\n",
      "Loss at step 706 : 0.0247\n",
      "Loss at step 707 : 0.0247\n",
      "Loss at step 708 : 0.0247\n",
      "Loss at step 709 : 0.0247\n",
      "Loss at step 710 : 0.0247\n",
      "Loss at step 711 : 0.0247\n",
      "Loss at step 712 : 0.0247\n",
      "Loss at step 713 : 0.0247\n",
      "Loss at step 714 : 0.0247\n",
      "Loss at step 715 : 0.0247\n",
      "Loss at step 716 : 0.0247\n",
      "Loss at step 717 : 0.0247\n",
      "Loss at step 718 : 0.0247\n",
      "Loss at step 719 : 0.0247\n",
      "Loss at step 720 : 0.0247\n",
      "Loss at step 721 : 0.0247\n",
      "Loss at step 722 : 0.0247\n",
      "Loss at step 723 : 0.0247\n",
      "Loss at step 724 : 0.0247\n",
      "Loss at step 725 : 0.0247\n",
      "Loss at step 726 : 0.0247\n",
      "Loss at step 727 : 0.0247\n",
      "Loss at step 728 : 0.0247\n",
      "Loss at step 729 : 0.0247\n",
      "Loss at step 730 : 0.0247\n",
      "Loss at step 731 : 0.0247\n",
      "Loss at step 732 : 0.0247\n",
      "Loss at step 733 : 0.0247\n",
      "Loss at step 734 : 0.0247\n",
      "Loss at step 735 : 0.0247\n",
      "Loss at step 736 : 0.0247\n",
      "Loss at step 737 : 0.0247\n",
      "Loss at step 738 : 0.0247\n",
      "Loss at step 739 : 0.0247\n",
      "Loss at step 740 : 0.0247\n",
      "Loss at step 741 : 0.0247\n",
      "Loss at step 742 : 0.0247\n",
      "Loss at step 743 : 0.0247\n",
      "Loss at step 744 : 0.0247\n",
      "Loss at step 745 : 0.0247\n",
      "Loss at step 746 : 0.0247\n",
      "Loss at step 747 : 0.0247\n",
      "Loss at step 748 : 0.0247\n",
      "Loss at step 749 : 0.0247\n",
      "Loss at step 750 : 0.0247\n",
      "Loss at step 751 : 0.0247\n",
      "Loss at step 752 : 0.0247\n",
      "Loss at step 753 : 0.0247\n",
      "Loss at step 754 : 0.0247\n",
      "Loss at step 755 : 0.0247\n",
      "Loss at step 756 : 0.0247\n",
      "Loss at step 757 : 0.0247\n",
      "Loss at step 758 : 0.0247\n",
      "Loss at step 759 : 0.0247\n",
      "Loss at step 760 : 0.0247\n",
      "Loss at step 761 : 0.0247\n",
      "Loss at step 762 : 0.0247\n",
      "Loss at step 763 : 0.0247\n",
      "Loss at step 764 : 0.0247\n",
      "Loss at step 765 : 0.0247\n",
      "Loss at step 766 : 0.0247\n",
      "Loss at step 767 : 0.0247\n",
      "Loss at step 768 : 0.0247\n",
      "Loss at step 769 : 0.0247\n",
      "Loss at step 770 : 0.0247\n",
      "Loss at step 771 : 0.0247\n",
      "Loss at step 772 : 0.0247\n",
      "Loss at step 773 : 0.0247\n",
      "Loss at step 774 : 0.0247\n",
      "Loss at step 775 : 0.0247\n",
      "Loss at step 776 : 0.0247\n",
      "Loss at step 777 : 0.0247\n",
      "Loss at step 778 : 0.0247\n",
      "Loss at step 779 : 0.0247\n",
      "Loss at step 780 : 0.0247\n",
      "Loss at step 781 : 0.0247\n",
      "Loss at step 782 : 0.0247\n",
      "Loss at step 783 : 0.0247\n",
      "Loss at step 784 : 0.0247\n",
      "Loss at step 785 : 0.0247\n",
      "Loss at step 786 : 0.0247\n",
      "Loss at step 787 : 0.0247\n",
      "Loss at step 788 : 0.0247\n",
      "Loss at step 789 : 0.0247\n",
      "Loss at step 790 : 0.0247\n",
      "Loss at step 791 : 0.0247\n",
      "Loss at step 792 : 0.0247\n",
      "Loss at step 793 : 0.0247\n",
      "Loss at step 794 : 0.0247\n",
      "Loss at step 795 : 0.0247\n",
      "Loss at step 796 : 0.0247\n",
      "Loss at step 797 : 0.0247\n",
      "Loss at step 798 : 0.0247\n",
      "Loss at step 799 : 0.0247\n",
      "Loss at step 800 : 0.0247\n",
      "Loss at step 801 : 0.0247\n",
      "Loss at step 802 : 0.0247\n",
      "Loss at step 803 : 0.0247\n",
      "Loss at step 804 : 0.0247\n",
      "Loss at step 805 : 0.0247\n",
      "Loss at step 806 : 0.0247\n",
      "Loss at step 807 : 0.0247\n",
      "Loss at step 808 : 0.0247\n",
      "Loss at step 809 : 0.0247\n",
      "Loss at step 810 : 0.0247\n",
      "Loss at step 811 : 0.0247\n",
      "Loss at step 812 : 0.0247\n",
      "Loss at step 813 : 0.0247\n",
      "Loss at step 814 : 0.0247\n",
      "Loss at step 815 : 0.0247\n",
      "Loss at step 816 : 0.0247\n",
      "Loss at step 817 : 0.0247\n",
      "Loss at step 818 : 0.0247\n",
      "Loss at step 819 : 0.0247\n",
      "Loss at step 820 : 0.0247\n",
      "Loss at step 821 : 0.0247\n",
      "Loss at step 822 : 0.0247\n",
      "Loss at step 823 : 0.0247\n",
      "Loss at step 824 : 0.0247\n",
      "Loss at step 825 : 0.0247\n",
      "Loss at step 826 : 0.0247\n",
      "Loss at step 827 : 0.0247\n",
      "Loss at step 828 : 0.0247\n",
      "Loss at step 829 : 0.0247\n",
      "Loss at step 830 : 0.0247\n",
      "Loss at step 831 : 0.0247\n",
      "Loss at step 832 : 0.0247\n",
      "Loss at step 833 : 0.0247\n",
      "Loss at step 834 : 0.0247\n",
      "Loss at step 835 : 0.0247\n",
      "Loss at step 836 : 0.0247\n",
      "Loss at step 837 : 0.0247\n",
      "Loss at step 838 : 0.0247\n",
      "Loss at step 839 : 0.0247\n",
      "Loss at step 840 : 0.0247\n",
      "Loss at step 841 : 0.0247\n",
      "Loss at step 842 : 0.0247\n",
      "Loss at step 843 : 0.0247\n",
      "Loss at step 844 : 0.0247\n",
      "Loss at step 845 : 0.0247\n",
      "Loss at step 846 : 0.0247\n",
      "Loss at step 847 : 0.0247\n",
      "Loss at step 848 : 0.0247\n",
      "Loss at step 849 : 0.0247\n",
      "Loss at step 850 : 0.0247\n",
      "Loss at step 851 : 0.0247\n",
      "Loss at step 852 : 0.0247\n",
      "Loss at step 853 : 0.0247\n",
      "Loss at step 854 : 0.0247\n",
      "Loss at step 855 : 0.0247\n",
      "Loss at step 856 : 0.0247\n",
      "Loss at step 857 : 0.0247\n",
      "Loss at step 858 : 0.0247\n",
      "Loss at step 859 : 0.0247\n",
      "Loss at step 860 : 0.0247\n",
      "Loss at step 861 : 0.0247\n",
      "Loss at step 862 : 0.0247\n",
      "Loss at step 863 : 0.0247\n",
      "Loss at step 864 : 0.0247\n",
      "Loss at step 865 : 0.0247\n",
      "Loss at step 866 : 0.0247\n",
      "Loss at step 867 : 0.0247\n",
      "Loss at step 868 : 0.0247\n",
      "Loss at step 869 : 0.0247\n",
      "Loss at step 870 : 0.0247\n",
      "Loss at step 871 : 0.0247\n",
      "Loss at step 872 : 0.0247\n",
      "Loss at step 873 : 0.0247\n",
      "Loss at step 874 : 0.0247\n",
      "Loss at step 875 : 0.0247\n",
      "Loss at step 876 : 0.0247\n",
      "Loss at step 877 : 0.0247\n",
      "Loss at step 878 : 0.0247\n",
      "Loss at step 879 : 0.0247\n",
      "Loss at step 880 : 0.0247\n",
      "Loss at step 881 : 0.0247\n",
      "Loss at step 882 : 0.0247\n",
      "Loss at step 883 : 0.0247\n",
      "Loss at step 884 : 0.0247\n",
      "Loss at step 885 : 0.0247\n",
      "Loss at step 886 : 0.0247\n",
      "Loss at step 887 : 0.0247\n",
      "Loss at step 888 : 0.0247\n",
      "Loss at step 889 : 0.0247\n",
      "Loss at step 890 : 0.0247\n",
      "Loss at step 891 : 0.0247\n",
      "Loss at step 892 : 0.0247\n",
      "Loss at step 893 : 0.0247\n",
      "Loss at step 894 : 0.0247\n",
      "Loss at step 895 : 0.0247\n",
      "Loss at step 896 : 0.0247\n",
      "Loss at step 897 : 0.0247\n",
      "Loss at step 898 : 0.0247\n",
      "Loss at step 899 : 0.0247\n",
      "Loss at step 900 : 0.0247\n",
      "Loss at step 901 : 0.0247\n",
      "Loss at step 902 : 0.0247\n",
      "Loss at step 903 : 0.0247\n",
      "Loss at step 904 : 0.0247\n",
      "Loss at step 905 : 0.0247\n",
      "Loss at step 906 : 0.0247\n",
      "Loss at step 907 : 0.0247\n",
      "Loss at step 908 : 0.0247\n",
      "Loss at step 909 : 0.0247\n",
      "Loss at step 910 : 0.0247\n",
      "Loss at step 911 : 0.0247\n",
      "Loss at step 912 : 0.0247\n",
      "Loss at step 913 : 0.0247\n",
      "Loss at step 914 : 0.0247\n",
      "Loss at step 915 : 0.0247\n",
      "Loss at step 916 : 0.0247\n",
      "Loss at step 917 : 0.0247\n",
      "Loss at step 918 : 0.0247\n",
      "Loss at step 919 : 0.0247\n",
      "Loss at step 920 : 0.0247\n",
      "Loss at step 921 : 0.0247\n",
      "Loss at step 922 : 0.0247\n",
      "Loss at step 923 : 0.0247\n",
      "Loss at step 924 : 0.0247\n",
      "Loss at step 925 : 0.0247\n",
      "Loss at step 926 : 0.0247\n",
      "Loss at step 927 : 0.0247\n",
      "Loss at step 928 : 0.0247\n",
      "Loss at step 929 : 0.0247\n",
      "Loss at step 930 : 0.0247\n",
      "Loss at step 931 : 0.0247\n",
      "Loss at step 932 : 0.0247\n",
      "Loss at step 933 : 0.0247\n",
      "Loss at step 934 : 0.0247\n",
      "Loss at step 935 : 0.0247\n",
      "Loss at step 936 : 0.0247\n",
      "Loss at step 937 : 0.0247\n",
      "Loss at step 938 : 0.0247\n",
      "Loss at step 939 : 0.0247\n",
      "Loss at step 940 : 0.0247\n",
      "Loss at step 941 : 0.0247\n",
      "Loss at step 942 : 0.0247\n",
      "Loss at step 943 : 0.0247\n",
      "Loss at step 944 : 0.0247\n",
      "Loss at step 945 : 0.0247\n",
      "Loss at step 946 : 0.0247\n",
      "Loss at step 947 : 0.0247\n",
      "Loss at step 948 : 0.0247\n",
      "Loss at step 949 : 0.0247\n",
      "Loss at step 950 : 0.0247\n",
      "Loss at step 951 : 0.0247\n",
      "Loss at step 952 : 0.0247\n",
      "Loss at step 953 : 0.0247\n",
      "Loss at step 954 : 0.0247\n",
      "Loss at step 955 : 0.0247\n",
      "Loss at step 956 : 0.0247\n",
      "Loss at step 957 : 0.0247\n",
      "Loss at step 958 : 0.0247\n",
      "Loss at step 959 : 0.0247\n",
      "Loss at step 960 : 0.0247\n",
      "Loss at step 961 : 0.0247\n",
      "Loss at step 962 : 0.0247\n",
      "Loss at step 963 : 0.0247\n",
      "Loss at step 964 : 0.0247\n",
      "Loss at step 965 : 0.0247\n",
      "Loss at step 966 : 0.0247\n",
      "Loss at step 967 : 0.0247\n",
      "Loss at step 968 : 0.0247\n",
      "Loss at step 969 : 0.0247\n",
      "Loss at step 970 : 0.0247\n",
      "Loss at step 971 : 0.0247\n",
      "Loss at step 972 : 0.0247\n",
      "Loss at step 973 : 0.0247\n",
      "Loss at step 974 : 0.0247\n",
      "Loss at step 975 : 0.0247\n",
      "Loss at step 976 : 0.0247\n",
      "Loss at step 977 : 0.0247\n",
      "Loss at step 978 : 0.0247\n",
      "Loss at step 979 : 0.0247\n",
      "Loss at step 980 : 0.0247\n",
      "Loss at step 981 : 0.0247\n",
      "Loss at step 982 : 0.0247\n",
      "Loss at step 983 : 0.0247\n",
      "Loss at step 984 : 0.0247\n",
      "Loss at step 985 : 0.0247\n",
      "Loss at step 986 : 0.0247\n",
      "Loss at step 987 : 0.0247\n",
      "Loss at step 988 : 0.0247\n",
      "Loss at step 989 : 0.0247\n",
      "Loss at step 990 : 0.0247\n",
      "Loss at step 991 : 0.0247\n",
      "Loss at step 992 : 0.0247\n",
      "Loss at step 993 : 0.0247\n",
      "Loss at step 994 : 0.0247\n",
      "Loss at step 995 : 0.0247\n",
      "Loss at step 996 : 0.0247\n",
      "Loss at step 997 : 0.0247\n",
      "Loss at step 998 : 0.0247\n",
      "Loss at step 999 : 0.0247\n"
     ]
    }
   ],
   "source": [
    "# Create linear classifier\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "W = tf.Variable(initial_value=tf.random.uniform(shape=(input_dim, output_dim)))\n",
    "b = tf.Variable(initial_value=tf.zeros((output_dim,)))\n",
    "\n",
    "# Forward pass\n",
    "def model(inputs):\n",
    "    return tf.matmul(inputs, W) + b\n",
    "\n",
    "# Loss function\n",
    "def square_loss(targets, predictions):\n",
    "    per_sample_losses = tf.square(targets - predictions)\n",
    "    return tf.reduce_mean(per_sample_losses)\n",
    "\n",
    "# Training step\n",
    "learning_rate = 0.1\n",
    "def training_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = square_loss(predictions, targets)\n",
    "    grad_loss_wrt_w, grad_loss_wrt_b = tape.gradient(loss, [W, b])\n",
    "    W.assign_sub(grad_loss_wrt_w * learning_rate)\n",
    "    b.assign_sub(grad_loss_wrt_b * learning_rate)\n",
    "    return loss\n",
    "\n",
    "# Batch training\n",
    "for step in range(1000):\n",
    "    loss = training_step(inputs, targets)\n",
    "    print(f\"Loss at step {step} : {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot predictions (with boundary line)\n",
    "x = np.linspace(-2,5,100)\n",
    "y = -W[0] / W[1] * x + (0.5-b)/W[1]\n",
    "predictions = model(inputs)\n",
    "plt.plot(x, y, \"-r\")\n",
    "plt.scatter(inputs[:,0], inputs[:,1], c=predictions[:,0] > 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 32)\n"
     ]
    }
   ],
   "source": [
    "# Dense layer as Layer subclass\n",
    "from tensorflow import keras\n",
    "\n",
    "class SimpleDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self.W = self.add_weight(shape=(input_dim, self.units),\n",
    "                                initializer=\"random_normal\")\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                initializer=\"zeros\")\n",
    "    def call(self, inputs):\n",
    "        y = tf.matmul(inputs, self.W) + self.b\n",
    "        if self.activation is not None:\n",
    "            y = self.activation(y)\n",
    "        return y\n",
    "\n",
    "# Test\n",
    "my_dense = SimpleDense(units=32, activation=tf.nn.relu)\n",
    "input_tensor = tf.ones(shape=(2,784))\n",
    "output_tensor = my_dense(input_tensor)\n",
    "print(output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 5.1930 - binary_accuracy: 0.7789 - val_loss: 4.4213 - val_binary_accuracy: 0.8050\n",
      "Epoch 2/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.0045 - binary_accuracy: 0.7861 - val_loss: 4.3032 - val_binary_accuracy: 0.8100\n",
      "Epoch 3/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4.8702 - binary_accuracy: 0.7900 - val_loss: 4.2040 - val_binary_accuracy: 0.8150\n",
      "Epoch 4/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4.7513 - binary_accuracy: 0.7950 - val_loss: 4.0966 - val_binary_accuracy: 0.8200\n",
      "Epoch 5/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4.6275 - binary_accuracy: 0.7983 - val_loss: 4.0078 - val_binary_accuracy: 0.8250\n",
      "Epoch 6/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.5198 - binary_accuracy: 0.8022 - val_loss: 3.9176 - val_binary_accuracy: 0.8250\n",
      "Epoch 7/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.4121 - binary_accuracy: 0.8078 - val_loss: 3.8189 - val_binary_accuracy: 0.8250\n",
      "Epoch 8/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.2973 - binary_accuracy: 0.8144 - val_loss: 3.7327 - val_binary_accuracy: 0.8350\n",
      "Epoch 9/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.1942 - binary_accuracy: 0.8178 - val_loss: 3.6502 - val_binary_accuracy: 0.8350\n",
      "Epoch 10/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 4.0953 - binary_accuracy: 0.8233 - val_loss: 3.5587 - val_binary_accuracy: 0.8400\n",
      "Epoch 11/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.9877 - binary_accuracy: 0.8278 - val_loss: 3.4711 - val_binary_accuracy: 0.8650\n",
      "Epoch 12/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.8851 - binary_accuracy: 0.8333 - val_loss: 3.3886 - val_binary_accuracy: 0.8650\n",
      "Epoch 13/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.7864 - binary_accuracy: 0.8389 - val_loss: 3.3155 - val_binary_accuracy: 0.8650\n",
      "Epoch 14/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6973 - binary_accuracy: 0.8444 - val_loss: 3.2366 - val_binary_accuracy: 0.8700\n",
      "Epoch 15/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.6026 - binary_accuracy: 0.8483 - val_loss: 3.1568 - val_binary_accuracy: 0.8700\n",
      "Epoch 16/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.5101 - binary_accuracy: 0.8517 - val_loss: 3.0864 - val_binary_accuracy: 0.8700\n",
      "Epoch 17/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.4265 - binary_accuracy: 0.8544 - val_loss: 3.0050 - val_binary_accuracy: 0.8750\n",
      "Epoch 18/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3.3328 - binary_accuracy: 0.8606 - val_loss: 2.9455 - val_binary_accuracy: 0.8800\n",
      "Epoch 19/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.2578 - binary_accuracy: 0.8656 - val_loss: 2.8801 - val_binary_accuracy: 0.8850\n",
      "Epoch 20/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.1789 - binary_accuracy: 0.8717 - val_loss: 2.8073 - val_binary_accuracy: 0.8900\n",
      "Epoch 21/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3.0964 - binary_accuracy: 0.8733 - val_loss: 2.7445 - val_binary_accuracy: 0.8950\n",
      "Epoch 22/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3.0239 - binary_accuracy: 0.8739 - val_loss: 2.6800 - val_binary_accuracy: 0.9000\n",
      "Epoch 23/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.9476 - binary_accuracy: 0.8761 - val_loss: 2.6158 - val_binary_accuracy: 0.9000\n",
      "Epoch 24/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.8711 - binary_accuracy: 0.8828 - val_loss: 2.5472 - val_binary_accuracy: 0.9050\n",
      "Epoch 25/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.7926 - binary_accuracy: 0.8883 - val_loss: 2.4894 - val_binary_accuracy: 0.9100\n",
      "Epoch 26/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.7242 - binary_accuracy: 0.8894 - val_loss: 2.4309 - val_binary_accuracy: 0.9100\n",
      "Epoch 27/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.6557 - binary_accuracy: 0.8922 - val_loss: 2.3722 - val_binary_accuracy: 0.9100\n",
      "Epoch 28/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.5906 - binary_accuracy: 0.8944 - val_loss: 2.3086 - val_binary_accuracy: 0.9100\n",
      "Epoch 29/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.5208 - binary_accuracy: 0.8950 - val_loss: 2.2536 - val_binary_accuracy: 0.9150\n",
      "Epoch 30/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.4591 - binary_accuracy: 0.8961 - val_loss: 2.1946 - val_binary_accuracy: 0.9200\n",
      "Epoch 31/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.3928 - binary_accuracy: 0.8983 - val_loss: 2.1372 - val_binary_accuracy: 0.9200\n",
      "Epoch 32/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.3296 - binary_accuracy: 0.8989 - val_loss: 2.0801 - val_binary_accuracy: 0.9250\n",
      "Epoch 33/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.2647 - binary_accuracy: 0.9017 - val_loss: 2.0233 - val_binary_accuracy: 0.9300\n",
      "Epoch 34/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.2012 - binary_accuracy: 0.9022 - val_loss: 1.9683 - val_binary_accuracy: 0.9300\n",
      "Epoch 35/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 2.1421 - binary_accuracy: 0.9022 - val_loss: 1.9148 - val_binary_accuracy: 0.9300\n",
      "Epoch 36/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.0813 - binary_accuracy: 0.9044 - val_loss: 1.8649 - val_binary_accuracy: 0.9300\n",
      "Epoch 37/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 2.0255 - binary_accuracy: 0.9056 - val_loss: 1.8108 - val_binary_accuracy: 0.9300\n",
      "Epoch 38/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.9695 - binary_accuracy: 0.9056 - val_loss: 1.7622 - val_binary_accuracy: 0.9300\n",
      "Epoch 39/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.9155 - binary_accuracy: 0.9067 - val_loss: 1.7150 - val_binary_accuracy: 0.9300\n",
      "Epoch 40/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.8620 - binary_accuracy: 0.9067 - val_loss: 1.6626 - val_binary_accuracy: 0.9300\n",
      "Epoch 41/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.8041 - binary_accuracy: 0.9094 - val_loss: 1.6170 - val_binary_accuracy: 0.9300\n",
      "Epoch 42/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.7526 - binary_accuracy: 0.9100 - val_loss: 1.5650 - val_binary_accuracy: 0.9350\n",
      "Epoch 43/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.6975 - binary_accuracy: 0.9094 - val_loss: 1.5167 - val_binary_accuracy: 0.9300\n",
      "Epoch 44/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.6480 - binary_accuracy: 0.9094 - val_loss: 1.4721 - val_binary_accuracy: 0.9350\n",
      "Epoch 45/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.5987 - binary_accuracy: 0.9094 - val_loss: 1.4244 - val_binary_accuracy: 0.9350\n",
      "Epoch 46/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.5489 - binary_accuracy: 0.9089 - val_loss: 1.3806 - val_binary_accuracy: 0.9300\n",
      "Epoch 47/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.5030 - binary_accuracy: 0.9089 - val_loss: 1.3400 - val_binary_accuracy: 0.9300\n",
      "Epoch 48/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.4604 - binary_accuracy: 0.9089 - val_loss: 1.2986 - val_binary_accuracy: 0.9350\n",
      "Epoch 49/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.4145 - binary_accuracy: 0.9089 - val_loss: 1.2529 - val_binary_accuracy: 0.9350\n",
      "Epoch 50/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.3666 - binary_accuracy: 0.9094 - val_loss: 1.2155 - val_binary_accuracy: 0.9350\n",
      "Epoch 51/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.3236 - binary_accuracy: 0.9122 - val_loss: 1.1772 - val_binary_accuracy: 0.9400\n",
      "Epoch 52/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.2809 - binary_accuracy: 0.9133 - val_loss: 1.1371 - val_binary_accuracy: 0.9400\n",
      "Epoch 53/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.2384 - binary_accuracy: 0.9117 - val_loss: 1.0992 - val_binary_accuracy: 0.9400\n",
      "Epoch 54/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.1982 - binary_accuracy: 0.9106 - val_loss: 1.0647 - val_binary_accuracy: 0.9400\n",
      "Epoch 55/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.1592 - binary_accuracy: 0.9111 - val_loss: 1.0256 - val_binary_accuracy: 0.9300\n",
      "Epoch 56/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.1171 - binary_accuracy: 0.9128 - val_loss: 0.9927 - val_binary_accuracy: 0.9300\n",
      "Epoch 57/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.0830 - binary_accuracy: 0.9117 - val_loss: 0.9612 - val_binary_accuracy: 0.9250\n",
      "Epoch 58/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.0478 - binary_accuracy: 0.9117 - val_loss: 0.9273 - val_binary_accuracy: 0.9300\n",
      "Epoch 59/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.0103 - binary_accuracy: 0.9106 - val_loss: 0.9032 - val_binary_accuracy: 0.9350\n",
      "Epoch 60/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.9809 - binary_accuracy: 0.9111 - val_loss: 0.8741 - val_binary_accuracy: 0.9400\n",
      "Epoch 61/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.9478 - binary_accuracy: 0.9122 - val_loss: 0.8435 - val_binary_accuracy: 0.9400\n",
      "Epoch 62/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.9142 - binary_accuracy: 0.9150 - val_loss: 0.8201 - val_binary_accuracy: 0.9400\n",
      "Epoch 63/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.8866 - binary_accuracy: 0.9156 - val_loss: 0.7952 - val_binary_accuracy: 0.9450\n",
      "Epoch 64/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.8585 - binary_accuracy: 0.9139 - val_loss: 0.7676 - val_binary_accuracy: 0.9450\n",
      "Epoch 65/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.8284 - binary_accuracy: 0.9133 - val_loss: 0.7418 - val_binary_accuracy: 0.9350\n",
      "Epoch 66/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.8022 - binary_accuracy: 0.9106 - val_loss: 0.7166 - val_binary_accuracy: 0.9350\n",
      "Epoch 67/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7736 - binary_accuracy: 0.9128 - val_loss: 0.6928 - val_binary_accuracy: 0.9350\n",
      "Epoch 68/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7478 - binary_accuracy: 0.9100 - val_loss: 0.6710 - val_binary_accuracy: 0.9250\n",
      "Epoch 69/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7241 - binary_accuracy: 0.9083 - val_loss: 0.6467 - val_binary_accuracy: 0.9150\n",
      "Epoch 70/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6974 - binary_accuracy: 0.9072 - val_loss: 0.6223 - val_binary_accuracy: 0.9150\n",
      "Epoch 71/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6723 - binary_accuracy: 0.9033 - val_loss: 0.5969 - val_binary_accuracy: 0.9050\n",
      "Epoch 72/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6477 - binary_accuracy: 0.9006 - val_loss: 0.5799 - val_binary_accuracy: 0.9000\n",
      "Epoch 73/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6293 - binary_accuracy: 0.8978 - val_loss: 0.5626 - val_binary_accuracy: 0.9000\n",
      "Epoch 74/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6097 - binary_accuracy: 0.8978 - val_loss: 0.5433 - val_binary_accuracy: 0.9050\n",
      "Epoch 75/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5881 - binary_accuracy: 0.8961 - val_loss: 0.5269 - val_binary_accuracy: 0.9000\n",
      "Epoch 76/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5698 - binary_accuracy: 0.8928 - val_loss: 0.5105 - val_binary_accuracy: 0.8900\n",
      "Epoch 77/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5526 - binary_accuracy: 0.8906 - val_loss: 0.4958 - val_binary_accuracy: 0.8950\n",
      "Epoch 78/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5356 - binary_accuracy: 0.8900 - val_loss: 0.4787 - val_binary_accuracy: 0.8850\n",
      "Epoch 79/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5171 - binary_accuracy: 0.8872 - val_loss: 0.4662 - val_binary_accuracy: 0.8900\n",
      "Epoch 80/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5010 - binary_accuracy: 0.8861 - val_loss: 0.4546 - val_binary_accuracy: 0.8950\n",
      "Epoch 81/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4864 - binary_accuracy: 0.8872 - val_loss: 0.4385 - val_binary_accuracy: 0.8850\n",
      "Epoch 82/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4700 - binary_accuracy: 0.8822 - val_loss: 0.4268 - val_binary_accuracy: 0.8900\n",
      "Epoch 83/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4568 - binary_accuracy: 0.8817 - val_loss: 0.4144 - val_binary_accuracy: 0.8950\n",
      "Epoch 84/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4428 - binary_accuracy: 0.8844 - val_loss: 0.3999 - val_binary_accuracy: 0.8900\n",
      "Epoch 85/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4275 - binary_accuracy: 0.8811 - val_loss: 0.3872 - val_binary_accuracy: 0.8900\n",
      "Epoch 86/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4142 - binary_accuracy: 0.8811 - val_loss: 0.3761 - val_binary_accuracy: 0.8950\n",
      "Epoch 87/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4005 - binary_accuracy: 0.8828 - val_loss: 0.3666 - val_binary_accuracy: 0.8900\n",
      "Epoch 88/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3880 - binary_accuracy: 0.8850 - val_loss: 0.3542 - val_binary_accuracy: 0.8900\n",
      "Epoch 89/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3757 - binary_accuracy: 0.8817 - val_loss: 0.3450 - val_binary_accuracy: 0.8900\n",
      "Epoch 90/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3655 - binary_accuracy: 0.8800 - val_loss: 0.3360 - val_binary_accuracy: 0.8900\n",
      "Epoch 91/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3548 - binary_accuracy: 0.8800 - val_loss: 0.3265 - val_binary_accuracy: 0.8850\n",
      "Epoch 92/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3434 - binary_accuracy: 0.8794 - val_loss: 0.3178 - val_binary_accuracy: 0.8850\n",
      "Epoch 93/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3328 - binary_accuracy: 0.8794 - val_loss: 0.3090 - val_binary_accuracy: 0.8850\n",
      "Epoch 94/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3239 - binary_accuracy: 0.8767 - val_loss: 0.3011 - val_binary_accuracy: 0.8850\n",
      "Epoch 95/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3143 - binary_accuracy: 0.8767 - val_loss: 0.2935 - val_binary_accuracy: 0.8900\n",
      "Epoch 96/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3054 - binary_accuracy: 0.8761 - val_loss: 0.2830 - val_binary_accuracy: 0.8850\n",
      "Epoch 97/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2951 - binary_accuracy: 0.8722 - val_loss: 0.2778 - val_binary_accuracy: 0.8900\n",
      "Epoch 98/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2876 - binary_accuracy: 0.8744 - val_loss: 0.2709 - val_binary_accuracy: 0.8900\n",
      "Epoch 99/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2794 - binary_accuracy: 0.8711 - val_loss: 0.2645 - val_binary_accuracy: 0.8850\n",
      "Epoch 100/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2718 - binary_accuracy: 0.8711 - val_loss: 0.2572 - val_binary_accuracy: 0.8850\n",
      "Epoch 101/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2644 - binary_accuracy: 0.8678 - val_loss: 0.2500 - val_binary_accuracy: 0.8750\n",
      "Epoch 102/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2567 - binary_accuracy: 0.8661 - val_loss: 0.2436 - val_binary_accuracy: 0.8700\n",
      "Epoch 103/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2497 - binary_accuracy: 0.8617 - val_loss: 0.2388 - val_binary_accuracy: 0.8700\n",
      "Epoch 104/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2429 - binary_accuracy: 0.8633 - val_loss: 0.2316 - val_binary_accuracy: 0.8650\n",
      "Epoch 105/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2358 - binary_accuracy: 0.8594 - val_loss: 0.2266 - val_binary_accuracy: 0.8650\n",
      "Epoch 106/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2307 - binary_accuracy: 0.8572 - val_loss: 0.2227 - val_binary_accuracy: 0.8550\n",
      "Epoch 107/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2258 - binary_accuracy: 0.8589 - val_loss: 0.2188 - val_binary_accuracy: 0.8650\n",
      "Epoch 108/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2204 - binary_accuracy: 0.8606 - val_loss: 0.2140 - val_binary_accuracy: 0.8650\n",
      "Epoch 109/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2154 - binary_accuracy: 0.8600 - val_loss: 0.2097 - val_binary_accuracy: 0.8650\n",
      "Epoch 110/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2103 - binary_accuracy: 0.8567 - val_loss: 0.2053 - val_binary_accuracy: 0.8650\n",
      "Epoch 111/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2063 - binary_accuracy: 0.8583 - val_loss: 0.2014 - val_binary_accuracy: 0.8650\n",
      "Epoch 112/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2023 - binary_accuracy: 0.8561 - val_loss: 0.1983 - val_binary_accuracy: 0.8650\n",
      "Epoch 113/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1989 - binary_accuracy: 0.8583 - val_loss: 0.1950 - val_binary_accuracy: 0.8650\n",
      "Epoch 114/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1953 - binary_accuracy: 0.8600 - val_loss: 0.1915 - val_binary_accuracy: 0.8650\n",
      "Epoch 115/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1917 - binary_accuracy: 0.8583 - val_loss: 0.1882 - val_binary_accuracy: 0.8700\n",
      "Epoch 116/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1878 - binary_accuracy: 0.8617 - val_loss: 0.1852 - val_binary_accuracy: 0.8600\n",
      "Epoch 117/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1840 - binary_accuracy: 0.8606 - val_loss: 0.1828 - val_binary_accuracy: 0.8600\n",
      "Epoch 118/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1808 - binary_accuracy: 0.8633 - val_loss: 0.1794 - val_binary_accuracy: 0.8600\n",
      "Epoch 119/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1775 - binary_accuracy: 0.8628 - val_loss: 0.1770 - val_binary_accuracy: 0.8700\n",
      "Epoch 120/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1747 - binary_accuracy: 0.8639 - val_loss: 0.1748 - val_binary_accuracy: 0.8700\n",
      "Epoch 121/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1720 - binary_accuracy: 0.8656 - val_loss: 0.1708 - val_binary_accuracy: 0.8700\n",
      "Epoch 122/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1687 - binary_accuracy: 0.8639 - val_loss: 0.1673 - val_binary_accuracy: 0.8700\n",
      "Epoch 123/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1657 - binary_accuracy: 0.8656 - val_loss: 0.1638 - val_binary_accuracy: 0.8700\n",
      "Epoch 124/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1630 - binary_accuracy: 0.8656 - val_loss: 0.1609 - val_binary_accuracy: 0.8850\n",
      "Epoch 125/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1603 - binary_accuracy: 0.8689 - val_loss: 0.1587 - val_binary_accuracy: 0.8950\n",
      "Epoch 126/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1577 - binary_accuracy: 0.8722 - val_loss: 0.1553 - val_binary_accuracy: 0.8950\n",
      "Epoch 127/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1551 - binary_accuracy: 0.8750 - val_loss: 0.1531 - val_binary_accuracy: 0.8950\n",
      "Epoch 128/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1524 - binary_accuracy: 0.8767 - val_loss: 0.1507 - val_binary_accuracy: 0.9000\n",
      "Epoch 129/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1496 - binary_accuracy: 0.8800 - val_loss: 0.1478 - val_binary_accuracy: 0.9000\n",
      "Epoch 130/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1469 - binary_accuracy: 0.8800 - val_loss: 0.1446 - val_binary_accuracy: 0.9000\n",
      "Epoch 131/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1443 - binary_accuracy: 0.8856 - val_loss: 0.1425 - val_binary_accuracy: 0.9000\n",
      "Epoch 132/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1416 - binary_accuracy: 0.8856 - val_loss: 0.1397 - val_binary_accuracy: 0.9000\n",
      "Epoch 133/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1388 - binary_accuracy: 0.8872 - val_loss: 0.1363 - val_binary_accuracy: 0.9100\n",
      "Epoch 134/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1363 - binary_accuracy: 0.8950 - val_loss: 0.1335 - val_binary_accuracy: 0.9100\n",
      "Epoch 135/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1336 - binary_accuracy: 0.8950 - val_loss: 0.1306 - val_binary_accuracy: 0.9150\n",
      "Epoch 136/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1309 - binary_accuracy: 0.8994 - val_loss: 0.1280 - val_binary_accuracy: 0.9150\n",
      "Epoch 137/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1285 - binary_accuracy: 0.9000 - val_loss: 0.1260 - val_binary_accuracy: 0.9150\n",
      "Epoch 138/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1263 - binary_accuracy: 0.9006 - val_loss: 0.1231 - val_binary_accuracy: 0.9150\n",
      "Epoch 139/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1238 - binary_accuracy: 0.9039 - val_loss: 0.1212 - val_binary_accuracy: 0.9150\n",
      "Epoch 140/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1215 - binary_accuracy: 0.9056 - val_loss: 0.1181 - val_binary_accuracy: 0.9150\n",
      "Epoch 141/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1189 - binary_accuracy: 0.9100 - val_loss: 0.1161 - val_binary_accuracy: 0.9150\n",
      "Epoch 142/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1166 - binary_accuracy: 0.9094 - val_loss: 0.1136 - val_binary_accuracy: 0.9200\n",
      "Epoch 143/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1143 - binary_accuracy: 0.9100 - val_loss: 0.1123 - val_binary_accuracy: 0.9200\n",
      "Epoch 144/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1123 - binary_accuracy: 0.9128 - val_loss: 0.1091 - val_binary_accuracy: 0.9250\n",
      "Epoch 145/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1101 - binary_accuracy: 0.9167 - val_loss: 0.1066 - val_binary_accuracy: 0.9250\n",
      "Epoch 146/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1080 - binary_accuracy: 0.9178 - val_loss: 0.1047 - val_binary_accuracy: 0.9250\n",
      "Epoch 147/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1059 - binary_accuracy: 0.9178 - val_loss: 0.1018 - val_binary_accuracy: 0.9250\n",
      "Epoch 148/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1038 - binary_accuracy: 0.9211 - val_loss: 0.0996 - val_binary_accuracy: 0.9250\n",
      "Epoch 149/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1018 - binary_accuracy: 0.9244 - val_loss: 0.0981 - val_binary_accuracy: 0.9350\n",
      "Epoch 150/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0998 - binary_accuracy: 0.9283 - val_loss: 0.0961 - val_binary_accuracy: 0.9350\n",
      "Epoch 151/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0978 - binary_accuracy: 0.9278 - val_loss: 0.0934 - val_binary_accuracy: 0.9400\n",
      "Epoch 152/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0959 - binary_accuracy: 0.9300 - val_loss: 0.0919 - val_binary_accuracy: 0.9400\n",
      "Epoch 153/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0939 - binary_accuracy: 0.9311 - val_loss: 0.0905 - val_binary_accuracy: 0.9400\n",
      "Epoch 154/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0921 - binary_accuracy: 0.9333 - val_loss: 0.0884 - val_binary_accuracy: 0.9400\n",
      "Epoch 155/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0902 - binary_accuracy: 0.9367 - val_loss: 0.0859 - val_binary_accuracy: 0.9400\n",
      "Epoch 156/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0881 - binary_accuracy: 0.9344 - val_loss: 0.0838 - val_binary_accuracy: 0.9400\n",
      "Epoch 157/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0862 - binary_accuracy: 0.9372 - val_loss: 0.0816 - val_binary_accuracy: 0.9450\n",
      "Epoch 158/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0843 - binary_accuracy: 0.9394 - val_loss: 0.0797 - val_binary_accuracy: 0.9500\n",
      "Epoch 159/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0825 - binary_accuracy: 0.9456 - val_loss: 0.0776 - val_binary_accuracy: 0.9550\n",
      "Epoch 160/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0807 - binary_accuracy: 0.9494 - val_loss: 0.0757 - val_binary_accuracy: 0.9550\n",
      "Epoch 161/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0790 - binary_accuracy: 0.9506 - val_loss: 0.0743 - val_binary_accuracy: 0.9600\n",
      "Epoch 162/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0772 - binary_accuracy: 0.9539 - val_loss: 0.0723 - val_binary_accuracy: 0.9700\n",
      "Epoch 163/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0755 - binary_accuracy: 0.9561 - val_loss: 0.0704 - val_binary_accuracy: 0.9700\n",
      "Epoch 164/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0737 - binary_accuracy: 0.9561 - val_loss: 0.0686 - val_binary_accuracy: 0.9700\n",
      "Epoch 165/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0722 - binary_accuracy: 0.9594 - val_loss: 0.0673 - val_binary_accuracy: 0.9700\n",
      "Epoch 166/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0706 - binary_accuracy: 0.9600 - val_loss: 0.0662 - val_binary_accuracy: 0.9700\n",
      "Epoch 167/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0690 - binary_accuracy: 0.9600 - val_loss: 0.0641 - val_binary_accuracy: 0.9700\n",
      "Epoch 168/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0674 - binary_accuracy: 0.9628 - val_loss: 0.0625 - val_binary_accuracy: 0.9750\n",
      "Epoch 169/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0657 - binary_accuracy: 0.9644 - val_loss: 0.0614 - val_binary_accuracy: 0.9750\n",
      "Epoch 170/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0643 - binary_accuracy: 0.9694 - val_loss: 0.0597 - val_binary_accuracy: 0.9900\n",
      "Epoch 171/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0627 - binary_accuracy: 0.9700 - val_loss: 0.0580 - val_binary_accuracy: 0.9950\n",
      "Epoch 172/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0612 - binary_accuracy: 0.9728 - val_loss: 0.0564 - val_binary_accuracy: 0.9950\n",
      "Epoch 173/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0599 - binary_accuracy: 0.9744 - val_loss: 0.0547 - val_binary_accuracy: 1.0000\n",
      "Epoch 174/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0587 - binary_accuracy: 0.9750 - val_loss: 0.0536 - val_binary_accuracy: 1.0000\n",
      "Epoch 175/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0572 - binary_accuracy: 0.9756 - val_loss: 0.0524 - val_binary_accuracy: 1.0000\n",
      "Epoch 176/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0558 - binary_accuracy: 0.9767 - val_loss: 0.0504 - val_binary_accuracy: 1.0000\n",
      "Epoch 177/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0545 - binary_accuracy: 0.9778 - val_loss: 0.0495 - val_binary_accuracy: 1.0000\n",
      "Epoch 178/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0533 - binary_accuracy: 0.9783 - val_loss: 0.0482 - val_binary_accuracy: 1.0000\n",
      "Epoch 179/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0521 - binary_accuracy: 0.9789 - val_loss: 0.0467 - val_binary_accuracy: 1.0000\n",
      "Epoch 180/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0508 - binary_accuracy: 0.9789 - val_loss: 0.0457 - val_binary_accuracy: 1.0000\n",
      "Epoch 181/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0496 - binary_accuracy: 0.9800 - val_loss: 0.0439 - val_binary_accuracy: 1.0000\n",
      "Epoch 182/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0484 - binary_accuracy: 0.9806 - val_loss: 0.0430 - val_binary_accuracy: 1.0000\n",
      "Epoch 183/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0474 - binary_accuracy: 0.9828 - val_loss: 0.0421 - val_binary_accuracy: 1.0000\n",
      "Epoch 184/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0463 - binary_accuracy: 0.9833 - val_loss: 0.0406 - val_binary_accuracy: 1.0000\n",
      "Epoch 185/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0452 - binary_accuracy: 0.9833 - val_loss: 0.0399 - val_binary_accuracy: 1.0000\n",
      "Epoch 186/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0442 - binary_accuracy: 0.9844 - val_loss: 0.0387 - val_binary_accuracy: 1.0000\n",
      "Epoch 187/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0433 - binary_accuracy: 0.9856 - val_loss: 0.0377 - val_binary_accuracy: 1.0000\n",
      "Epoch 188/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0425 - binary_accuracy: 0.9867 - val_loss: 0.0367 - val_binary_accuracy: 1.0000\n",
      "Epoch 189/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0414 - binary_accuracy: 0.9878 - val_loss: 0.0355 - val_binary_accuracy: 1.0000\n",
      "Epoch 190/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0406 - binary_accuracy: 0.9883 - val_loss: 0.0352 - val_binary_accuracy: 1.0000\n",
      "Epoch 191/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0398 - binary_accuracy: 0.9894 - val_loss: 0.0343 - val_binary_accuracy: 1.0000\n",
      "Epoch 192/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0390 - binary_accuracy: 0.9906 - val_loss: 0.0341 - val_binary_accuracy: 1.0000\n",
      "Epoch 193/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0385 - binary_accuracy: 0.9911 - val_loss: 0.0331 - val_binary_accuracy: 1.0000\n",
      "Epoch 194/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0377 - binary_accuracy: 0.9906 - val_loss: 0.0320 - val_binary_accuracy: 1.0000\n",
      "Epoch 195/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0370 - binary_accuracy: 0.9911 - val_loss: 0.0310 - val_binary_accuracy: 1.0000\n",
      "Epoch 196/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0363 - binary_accuracy: 0.9911 - val_loss: 0.0304 - val_binary_accuracy: 1.0000\n",
      "Epoch 197/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0357 - binary_accuracy: 0.9917 - val_loss: 0.0296 - val_binary_accuracy: 1.0000\n",
      "Epoch 198/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0351 - binary_accuracy: 0.9922 - val_loss: 0.0294 - val_binary_accuracy: 1.0000\n",
      "Epoch 199/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0344 - binary_accuracy: 0.9928 - val_loss: 0.0283 - val_binary_accuracy: 1.0000\n",
      "Epoch 200/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0338 - binary_accuracy: 0.9939 - val_loss: 0.0276 - val_binary_accuracy: 1.0000\n",
      "Epoch 201/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0332 - binary_accuracy: 0.9950 - val_loss: 0.0270 - val_binary_accuracy: 1.0000\n",
      "Epoch 202/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0326 - binary_accuracy: 0.9950 - val_loss: 0.0269 - val_binary_accuracy: 1.0000\n",
      "Epoch 203/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0320 - binary_accuracy: 0.9961 - val_loss: 0.0262 - val_binary_accuracy: 1.0000\n",
      "Epoch 204/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0315 - binary_accuracy: 0.9961 - val_loss: 0.0259 - val_binary_accuracy: 1.0000\n",
      "Epoch 205/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0310 - binary_accuracy: 0.9961 - val_loss: 0.0253 - val_binary_accuracy: 1.0000\n",
      "Epoch 206/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0306 - binary_accuracy: 0.9961 - val_loss: 0.0253 - val_binary_accuracy: 1.0000\n",
      "Epoch 207/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0303 - binary_accuracy: 0.9961 - val_loss: 0.0243 - val_binary_accuracy: 1.0000\n",
      "Epoch 208/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0298 - binary_accuracy: 0.9961 - val_loss: 0.0239 - val_binary_accuracy: 1.0000\n",
      "Epoch 209/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0294 - binary_accuracy: 0.9967 - val_loss: 0.0238 - val_binary_accuracy: 1.0000\n",
      "Epoch 210/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0291 - binary_accuracy: 0.9961 - val_loss: 0.0235 - val_binary_accuracy: 1.0000\n",
      "Epoch 211/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0288 - binary_accuracy: 0.9961 - val_loss: 0.0226 - val_binary_accuracy: 1.0000\n",
      "Epoch 212/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0284 - binary_accuracy: 0.9967 - val_loss: 0.0225 - val_binary_accuracy: 1.0000\n",
      "Epoch 213/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0281 - binary_accuracy: 0.9967 - val_loss: 0.0223 - val_binary_accuracy: 1.0000\n",
      "Epoch 214/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0278 - binary_accuracy: 0.9967 - val_loss: 0.0219 - val_binary_accuracy: 1.0000\n",
      "Epoch 215/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0275 - binary_accuracy: 0.9967 - val_loss: 0.0215 - val_binary_accuracy: 1.0000\n",
      "Epoch 216/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0274 - binary_accuracy: 0.9972 - val_loss: 0.0212 - val_binary_accuracy: 1.0000\n",
      "Epoch 217/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0272 - binary_accuracy: 0.9972 - val_loss: 0.0211 - val_binary_accuracy: 1.0000\n",
      "Epoch 218/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0270 - binary_accuracy: 0.9972 - val_loss: 0.0212 - val_binary_accuracy: 1.0000\n",
      "Epoch 219/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0268 - binary_accuracy: 0.9972 - val_loss: 0.0206 - val_binary_accuracy: 1.0000\n",
      "Epoch 220/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0266 - binary_accuracy: 0.9972 - val_loss: 0.0205 - val_binary_accuracy: 1.0000\n",
      "Epoch 221/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0264 - binary_accuracy: 0.9972 - val_loss: 0.0202 - val_binary_accuracy: 1.0000\n",
      "Epoch 222/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0263 - binary_accuracy: 0.9972 - val_loss: 0.0203 - val_binary_accuracy: 1.0000\n",
      "Epoch 223/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0262 - binary_accuracy: 0.9972 - val_loss: 0.0202 - val_binary_accuracy: 1.0000\n",
      "Epoch 224/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0261 - binary_accuracy: 0.9983 - val_loss: 0.0199 - val_binary_accuracy: 1.0000\n",
      "Epoch 225/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0260 - binary_accuracy: 0.9983 - val_loss: 0.0199 - val_binary_accuracy: 1.0000\n",
      "Epoch 226/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0259 - binary_accuracy: 0.9983 - val_loss: 0.0198 - val_binary_accuracy: 1.0000\n",
      "Epoch 227/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0258 - binary_accuracy: 0.9983 - val_loss: 0.0202 - val_binary_accuracy: 1.0000\n",
      "Epoch 228/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0259 - binary_accuracy: 0.9978 - val_loss: 0.0198 - val_binary_accuracy: 1.0000\n",
      "Epoch 229/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0257 - binary_accuracy: 0.9983 - val_loss: 0.0196 - val_binary_accuracy: 1.0000\n",
      "Epoch 230/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0257 - binary_accuracy: 0.9983 - val_loss: 0.0197 - val_binary_accuracy: 1.0000\n",
      "Epoch 231/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0256 - binary_accuracy: 0.9983 - val_loss: 0.0197 - val_binary_accuracy: 1.0000\n",
      "Epoch 232/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0256 - binary_accuracy: 0.9983 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 233/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0256 - binary_accuracy: 0.9983 - val_loss: 0.0197 - val_binary_accuracy: 1.0000\n",
      "Epoch 234/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0255 - binary_accuracy: 0.9983 - val_loss: 0.0198 - val_binary_accuracy: 1.0000\n",
      "Epoch 235/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0255 - binary_accuracy: 0.9983 - val_loss: 0.0199 - val_binary_accuracy: 1.0000\n",
      "Epoch 236/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0255 - binary_accuracy: 0.9983 - val_loss: 0.0198 - val_binary_accuracy: 1.0000\n",
      "Epoch 237/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0255 - binary_accuracy: 0.9983 - val_loss: 0.0197 - val_binary_accuracy: 1.0000\n",
      "Epoch 238/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0255 - binary_accuracy: 0.9983 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 239/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0197 - val_binary_accuracy: 1.0000\n",
      "Epoch 240/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0255 - binary_accuracy: 0.9989 - val_loss: 0.0197 - val_binary_accuracy: 1.0000\n",
      "Epoch 241/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9983 - val_loss: 0.0197 - val_binary_accuracy: 1.0000\n",
      "Epoch 242/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 243/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 244/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 245/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 246/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 247/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 248/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 249/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 250/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 251/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 252/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 253/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 254/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 255/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 256/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 257/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 258/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 259/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 260/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0254 - binary_accuracy: 0.9978 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 261/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 262/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 263/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 264/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 265/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0254 - binary_accuracy: 0.9978 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 266/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 267/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 268/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0196 - val_binary_accuracy: 1.0000\n",
      "Epoch 269/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 270/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 271/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 272/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 273/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 274/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 275/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0196 - val_binary_accuracy: 1.0000\n",
      "Epoch 276/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 277/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 278/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 279/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0254 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 280/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 281/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 282/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 283/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 284/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 285/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 286/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 287/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 288/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0197 - val_binary_accuracy: 1.0000\n",
      "Epoch 289/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 290/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 291/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 292/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 293/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 294/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 295/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 296/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 297/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 298/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 299/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 300/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 301/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 302/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 303/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 304/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 305/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 306/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 307/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 308/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 309/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 310/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 311/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 312/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 313/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0197 - val_binary_accuracy: 1.0000\n",
      "Epoch 314/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 315/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0196 - val_binary_accuracy: 1.0000\n",
      "Epoch 316/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 317/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 318/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 319/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 320/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 321/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 322/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 323/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 324/500\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 325/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 326/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 327/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 328/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 329/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 330/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 331/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0196 - val_binary_accuracy: 1.0000\n",
      "Epoch 332/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 333/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 334/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 335/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 336/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 337/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 338/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9983 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 339/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 340/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 341/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 342/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 343/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 344/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 345/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 346/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 347/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0196 - val_binary_accuracy: 1.0000\n",
      "Epoch 348/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 349/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 350/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 351/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 352/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0196 - val_binary_accuracy: 1.0000\n",
      "Epoch 353/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0196 - val_binary_accuracy: 1.0000\n",
      "Epoch 354/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0196 - val_binary_accuracy: 1.0000\n",
      "Epoch 355/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 356/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 357/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 358/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 359/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 360/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 361/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0254 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 362/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9978 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 363/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 364/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 365/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 366/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 367/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0254 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 368/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 369/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 370/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 371/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 372/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 373/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 374/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 375/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 376/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9978 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 377/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 378/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 379/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 380/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 381/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 382/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0254 - binary_accuracy: 0.9978 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 383/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 384/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 385/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 386/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0196 - val_binary_accuracy: 1.0000\n",
      "Epoch 387/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 388/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 389/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 390/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 391/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 392/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 393/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 394/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 395/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 396/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 397/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 398/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0196 - val_binary_accuracy: 1.0000\n",
      "Epoch 399/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 400/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 401/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 402/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 403/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 404/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 405/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9978 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 406/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 407/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 408/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 409/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 410/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 411/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 412/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 413/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 414/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 415/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 416/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 417/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 418/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 419/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 420/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 421/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 422/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 423/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 424/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 425/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 426/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 427/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 428/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0254 - binary_accuracy: 0.9978 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 429/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9978 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 430/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 431/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 432/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 433/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0190 - val_binary_accuracy: 1.0000\n",
      "Epoch 434/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 435/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 436/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 437/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 438/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 439/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 440/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 441/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 442/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 443/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 444/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 445/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 446/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 447/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9978 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 448/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 449/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n",
      "Epoch 450/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0196 - val_binary_accuracy: 1.0000\n",
      "Epoch 451/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 452/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 453/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 454/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 455/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 456/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 457/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9978 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 458/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 459/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 460/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 461/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 462/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 463/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0196 - val_binary_accuracy: 1.0000\n",
      "Epoch 464/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 465/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 466/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 467/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 468/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 469/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9978 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 470/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 471/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 472/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 473/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 474/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 475/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 476/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 477/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 478/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 479/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 480/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 481/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0196 - val_binary_accuracy: 1.0000\n",
      "Epoch 482/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0196 - val_binary_accuracy: 1.0000\n",
      "Epoch 483/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0254 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 484/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 485/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 486/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 487/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 488/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 489/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 490/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 491/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 492/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 493/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 494/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 495/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0191 - val_binary_accuracy: 1.0000\n",
      "Epoch 496/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0192 - val_binary_accuracy: 1.0000\n",
      "Epoch 497/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9983 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 498/500\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0193 - val_binary_accuracy: 1.0000\n",
      "Epoch 499/500\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0253 - binary_accuracy: 0.9978 - val_loss: 0.0194 - val_binary_accuracy: 1.0000\n",
      "Epoch 500/500\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0253 - binary_accuracy: 0.9989 - val_loss: 0.0195 - val_binary_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15ec7b34eb0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solving with keras (i.e. keras example)\n",
    "model = keras.Sequential([keras.layers.Dense(1)])\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "            loss=keras.losses.MeanSquaredError(),\n",
    "            metrics=[keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "indices_permutation = np.random.permutation(len(inputs))\n",
    "shuffled_inputs = inputs[indices_permutation]\n",
    "shuffled_targets = targets[indices_permutation]\n",
    "num_validation_samples = int(0.1 * len(inputs))\n",
    "val_inputs = shuffled_inputs[:num_validation_samples]\n",
    "val_targets = shuffled_targets[:num_validation_samples]\n",
    "training_inputs = shuffled_inputs[num_validation_samples:]\n",
    "training_targets = shuffled_targets[num_validation_samples:]\n",
    "model.fit(training_inputs,\n",
    "        training_targets,\n",
    "        epochs=500,\n",
    "        batch_size=256,\n",
    "        validation_data=(val_inputs, val_targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0195 - binary_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "loss_and_metrics = model.evaluate(val_inputs, val_targets, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.019482625648379326, 1.0]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_and_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "903db93db347b6cc4711a3ce3dd580256d2eecd4e3d421c66e7174fdec8ffda9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
